{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hired-stand",
   "metadata": {},
   "source": [
    "# Hierarchical aggregation of temporal data at scale using Dask\n",
    "\n",
    "Hierarchical aggregation allows for efficient computation of mean, SD, min, and max aggregates at varying time resolutions, by resuing previous results and avoiding processing raw data as much as possible. The core idea is to store intermediate (\"lossless\") aggregates, which can be used to compute the final aggregates (mean, SD, min, and max) at current or coarser time resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-burst",
   "metadata": {},
   "source": [
    "## Code framework of hierarchical aggregation\n",
    "\n",
    "The framework contains utility tools for operating on dataframes (`rename`, `get_vars_from_flat_cols`, and `get_cols`), and three main functions for producing the aggregates `init_agg`, `coarsen_agg`, and `finalize_agg`.\n",
    "\n",
    "### `init_agg` : raw → lossless\n",
    "`init_agg` is the only function to apply to raw temporal data. It produces intermediate (\"lossless\") aggregates at the requested time resolution. In an ideal use case, `init_agg` is called only once and for the finest time resolution possibly required. Its outputs are to be stored and subsequently used to compute the final aggregates, including for any coarser time resolution.\n",
    "\n",
    "### `coarsen_agg` : lossless → lossless\n",
    "`coarsen_agg` takes \"lossless\" aggregates and aggregates them further to some coarser time resolution. For precision, the latter should ideally be a multiple of the time resolution in the input data (e.g. coarsening aggregates at 5-second resolution into 1-minute resolution). For best performance, always choose the coarsest time resolution for the input (e.g. it's faster to obtain 1-minute aggregates from 30-second aggregates than from 5-second aggregates).\n",
    "\n",
    "### `finalize_agg` : lossless → final\n",
    "`finalize_agg` takes \"lossless\" aggregates and produces the final aggregates (mean, SD, min, and max) for the same time resolution. The final aggregates cannot be used for further coarsening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "million-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from itertools import chain\n",
    "from copy import copy\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "\n",
    "def rename(df):\n",
    "    '''\n",
    "    Rename columns to allow element-wise operations between dataframes of the same size.\n",
    "\n",
    "    df: dataframe\n",
    "    '''\n",
    "    return df.rename(columns={col: i for i, col in enumerate(df.columns)})\n",
    "\n",
    "\n",
    "def get_vars_from_flat_cols(df, not_vars=[]):\n",
    "    '''\n",
    "    Extract variable names as the first part of flat column names.\n",
    "    \n",
    "    df: dataframe\n",
    "    not_vars: list of columns that should not be included as variables\n",
    "    '''\n",
    "    return pd.unique(['.'.join(col.split('.')[:-1]) for col in df.columns if col not in not_vars])\n",
    "    \n",
    "    \n",
    "def get_cols(variables, agg_type):\n",
    "    '''\n",
    "    Produce column names for a particular aggregate type.\n",
    "    \n",
    "    variables: iterable with variable names\n",
    "    agg_type: aggregate type identifier, e.g. 'count' or 'sum_sq_diff'\n",
    "    '''\n",
    "    return [f'{x}.{agg_type}' for x in variables]\n",
    "    \n",
    "\n",
    "def init_agg(df_raw, time_res, grouping_cols=[]):\n",
    "    '''\n",
    "    \"Losslessly\" aggregate raw data to given time resolution.\n",
    "    \n",
    "    df_raw: time-indexed dataframe to aggregate\n",
    "    time_res: time resolution (e.g. '15s' or '2min')\n",
    "    grouping_cols: list of columns to group by\n",
    "    '''\n",
    "    # Extract names of variables to aggregate.\n",
    "    variables = [col for col in df_raw.columns if col not in grouping_cols]\n",
    "    \n",
    "    # Produce time column of the given time resolution.\n",
    "    time_col = df_raw.index.name\n",
    "    df_raw = df_raw.assign(**{time_col: lambda x: x.index.dt.floor(time_res).values})\n",
    "    df_raw[time_col] = dd.to_datetime(df_raw[time_col])\n",
    "    df_raw = df_raw.reset_index(drop=True)\n",
    "    \n",
    "    # Convert variable types to avoid overflow when computing variance.\n",
    "    df_raw[variables] = df_raw[variables].astype('float')\n",
    "    \n",
    "    # Produce intermediate aggregates at the given time resolution.\n",
    "    df_agg = df_raw.groupby(grouping_cols + [time_col]).agg(['count', 'sum', 'var', 'max', 'min'])\n",
    "\n",
    "    # Flatten column names of the intermediate aggregates.\n",
    "    cols = partial(get_cols, variables)\n",
    "    df_agg.columns = chain.from_iterable(\n",
    "        zip(cols('count'), cols('sum'), cols('sum_sq_diff'), cols('max'), cols('min')))\n",
    "\n",
    "    # Change variance to \"lossless\" sum of squared differences from mean.\n",
    "    sum_sq_diffs = rename(df_agg[cols('sum_sq_diff')])\n",
    "    counts = rename(df_agg[cols('count')])\n",
    "    df_agg[cols('sum_sq_diff')] = sum_sq_diffs.fillna(0) * (counts - 1)\n",
    "    \n",
    "    return df_agg\n",
    "\n",
    "\n",
    "def coarsen_agg(df_agg, time_res, time_col, grouping_cols=[]):\n",
    "    '''\n",
    "    Coarsen time resolution of \"losslessly\" aggregated data.\n",
    "    \n",
    "    df_agg: \"losslessly\" aggregated dataframe\n",
    "    time_res: new time resolution (e.g. '15s' or '2min'), at least as coarse as the current one\n",
    "    grouping_cols: list of columns to group by\n",
    "    '''\n",
    "    # Ensure that time_col and grouping_cols are contained in the columns.\n",
    "    if sum(1 for var in grouping_cols + [time_col] if var not in df_agg.columns) > 0:\n",
    "        df_coarse = df_agg.reset_index()\n",
    "    else:\n",
    "        df_coarse = copy(df_agg)\n",
    "\n",
    "    # Coarsen timestamp column to the given time resolution.\n",
    "    df_coarse[time_col] = df_coarse[time_col].dt.floor(time_res).values\n",
    "    \n",
    "    # Produce coarsened mean broadcasted to the original shape.\n",
    "    cols = partial(get_cols, get_vars_from_flat_cols(df_agg, not_vars=grouping_cols + [time_col]))\n",
    "    counts = rename(df_coarse[cols('count')])\n",
    "    sums = rename(df_coarse[cols('sum')])\n",
    "    df_coarse_grouped = df_coarse.groupby(grouping_cols + [time_col])\n",
    "    coarse_counts = rename(df_coarse_grouped[cols('count')].transform(\n",
    "        'sum', meta=df_coarse[cols('count')]._meta))\n",
    "    coarse_sums = rename(df_coarse_grouped[cols('sum')].transform(\n",
    "        'sum', meta=df_coarse[cols('sum')]._meta))\n",
    "    coarse_means = coarse_sums / coarse_counts\n",
    "    \n",
    "    # Produce sum of squared differences from coarsened mean.\n",
    "    sum_sq_diffs = rename(df_coarse[cols('sum_sq_diff')])\n",
    "    df_coarse[cols('sum_sq_diff')] = sum_sq_diffs + counts * (sums / counts - coarse_means)**2\n",
    "    \n",
    "    # Coarsen the \"lossless\" aggregates.\n",
    "    cols_to_agg = chain.from_iterable(\n",
    "        zip(cols('count'), cols('sum'), cols('sum_sq_diff'), cols('min'), cols('max')))\n",
    "    agg_functions = ['sum', 'sum', 'sum', 'min', 'max'] * len(cols(''))\n",
    "    df_coarse = df_coarse.groupby(grouping_cols + [time_col]).agg(dict(zip(cols_to_agg, agg_functions)))\n",
    "\n",
    "    return df_coarse\n",
    "\n",
    "\n",
    "def finalize_agg(df_agg, time_col, grouping_cols=[]):\n",
    "    '''\n",
    "    Compute final aggregates from losslessly aggregated data.\n",
    "    \n",
    "    df_agg: \"losslessly\" aggregated dataframe\n",
    "    grouping_cols: list of columns to group by\n",
    "    '''\n",
    "    # Ensure that time_col and grouping_cols are contained in the columns.\n",
    "    if sum(1 for var in grouping_cols + [time_col] if var not in df_agg.columns) > 0:\n",
    "        df_agg = df_agg.reset_index()\n",
    "    \n",
    "    # Produce means and standard deviations.\n",
    "    cols = partial(get_cols, get_vars_from_flat_cols(df_agg, not_vars=grouping_cols + [time_col]))\n",
    "    counts = rename(df_agg[cols('count')])\n",
    "    sums = rename(df_agg[cols('sum')])\n",
    "    sum_sq_diffs = rename(df_agg[cols('sum_sq_diff')])\n",
    "    means = (sums / counts).rename(columns={old: new for old, new in zip(counts.columns, cols('mean'))})\n",
    "    stds = ((sum_sq_diffs / (counts - 1))**.5).rename(\n",
    "        columns={old: new for old, new in zip(counts.columns, cols('std'))})\n",
    "    \n",
    "    # Compose final aggregates and reoder columns by variable.\n",
    "    df_final = dd.concat([df_agg[grouping_cols + [time_col]], means, stds, df_agg[cols('min') + cols('max')]],\n",
    "                         axis=1, ignore_unknown_divisions=True)\n",
    "    df_final = df_final[grouping_cols + [time_col] + list(\n",
    "        chain.from_iterable(zip(cols('mean'), cols('std'), cols('min'), cols('max'))))]\n",
    "    \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-november",
   "metadata": {},
   "source": [
    "## Setting up the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "innovative-mustang",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dashboard address for the dask-labextension\n",
      "/proxy/27961\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.43.202.87:42901</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.43.202.87:27961/status' target='_blank'>http://10.43.202.87:27961/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.43.202.87:42901' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from dask_jobqueue import SLURMCluster\n",
    "from distributed import Client\n",
    "\n",
    "\n",
    "# Set up Slurm cluster.\n",
    "dashboard_port = random.randint(10000,60000)\n",
    "cluster = SLURMCluster(scheduler_options={\"dashboard_address\": f\":{dashboard_port}\"})\n",
    "\n",
    "# We print out the address you copy into the dask-labextension\n",
    "print(\"Dashboard address for the dask-labextension\")\n",
    "print(f\"/proxy/{dashboard_port}\")\n",
    "\n",
    "# Create the client object\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "acting-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-staff",
   "metadata": {},
   "source": [
    "## Reading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "gorgeous-paradise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.2 s, sys: 1.12 s, total: 14.3 s\n",
      "Wall time: 2min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "import fastparquet as fp\n",
    "\n",
    "# Ensure temporary scratch space for this example.\n",
    "TMP = f'{os.environ[\"MEMBERWORK\"]}/gen150/.gears/gears/examples'\n",
    "\n",
    "# Read raw data.\n",
    "RAW_DATA = '/gpfs/alpine/stf218/proj-shared/stf008stc/openbmc.summit.raw/openbmc-20200429-*.parquet'\n",
    "df_raw = dd.read_parquet(\n",
    "    RAW_DATA, engine='fastparquet', index='timestamp',\n",
    "    columns=['hostname', 'total_power', 'ambient'], gather_statistics=False, chunksize='100MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-directive",
   "metadata": {},
   "source": [
    "## Hierarchical aggregation\n",
    "\n",
    "### Compute 15-second \"lossless\" aggregates from raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "major-design",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 69.8 ms, sys: 3.05 ms, total: 72.9 ms\n",
      "Wall time: 70.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_agg = init_agg(df_raw, '15s', grouping_cols=['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-reproduction",
   "metadata": {},
   "source": [
    "### Store 15-second \"lossless\" aggregates to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "defined-riverside",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.9 s, sys: 1.35 s, total: 25.2 s\n",
      "Wall time: 4min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "AGG_15S_LOSSLESS = f'{TMP}/openbmc.summit.15s.lossless'\n",
    "os.makedirs(AGG_15S_LOSSLESS, exist_ok=True)\n",
    "\n",
    "# Reset index because Dask won't be able to recover the entire multi-index.\n",
    "df_agg.reset_index().to_parquet(AGG_15S_LOSSLESS, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-mustang",
   "metadata": {},
   "source": [
    "### Compute 5-minute \"lossless\" aggregates from the multi-indexed output of `init_agg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "suffering-pontiac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 241 ms, sys: 13 ms, total: 254 ms\n",
      "Wall time: 250 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_coarse = coarsen_agg(df_agg, '5min', 'timestamp', grouping_cols=['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noted-excerpt",
   "metadata": {},
   "source": [
    "### ...or from its time-indexed disk version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "acoustic-switch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 129 ms, sys: 6.98 ms, total: 136 ms\n",
      "Wall time: 257 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_agg = dd.read_parquet(\n",
    "    AGG_15S_LOSSLESS, engine='fastparquet', index=False, gather_statistics=False, chunksize='100MB')\n",
    "df_coarse = coarsen_agg(df_agg, '5min', 'timestamp', grouping_cols=['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-regression",
   "metadata": {},
   "source": [
    "### Store 5-minute \"lossless\" aggregates to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aggregate-advancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.38 s, sys: 101 ms, total: 1.48 s\n",
      "Wall time: 26.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "AGG_5MIN_LOSSLESS = f'{TMP}/openbmc.summit.15s.lossless'\n",
    "os.makedirs(AGG_5MIN_LOSSLESS, exist_ok=True)\n",
    "\n",
    "df_coarse.reset_index().to_parquet(AGG_5MIN_LOSSLESS, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infectious-bracket",
   "metadata": {},
   "source": [
    "### Compute final aggregates at 5-minute resolution from the multi-indexed output of `coarse_agg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "nasty-pleasure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.7 ms, sys: 1.99 ms, total: 29.7 ms\n",
      "Wall time: 29 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_final = finalize_agg(df_coarse, 'timestamp', grouping_cols=['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-fantasy",
   "metadata": {},
   "source": [
    "### ...or from its time-indexed disk version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "harmful-baseball",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.5 ms, sys: 1.02 ms, total: 30.5 ms\n",
      "Wall time: 143 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_coarse = dd.read_parquet(\n",
    "    AGG_5MIN_LOSSLESS, engine='fastparquet', index=False, gather_statistics=False, chunksize='100MB')\n",
    "df_final = finalize_agg(df_coarse, 'timestamp', grouping_cols=['hostname'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-benefit",
   "metadata": {},
   "source": [
    "### Store 5-minute final aggregates to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "third-above",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 99.3 ms, sys: 5.83 ms, total: 105 ms\n",
      "Wall time: 1.76 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "AGG_5MIN = f'{TMP}/openbmc.summit.5min'\n",
    "os.makedirs(AGG_5MIN, exist_ok=True)\n",
    "\n",
    "df_final.reset_index().to_parquet(AGG_5MIN, engine='fastparquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-disabled",
   "metadata": {},
   "source": [
    "### Check the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "discrete-admission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostname</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>total_power.mean</th>\n",
       "      <th>total_power.std</th>\n",
       "      <th>total_power.min</th>\n",
       "      <th>total_power.max</th>\n",
       "      <th>ambient.mean</th>\n",
       "      <th>ambient.std</th>\n",
       "      <th>ambient.min</th>\n",
       "      <th>ambient.max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a01n01</td>\n",
       "      <td>2020-04-29 00:00:00</td>\n",
       "      <td>1077.569811</td>\n",
       "      <td>540.773446</td>\n",
       "      <td>566.0</td>\n",
       "      <td>2231.0</td>\n",
       "      <td>22.781132</td>\n",
       "      <td>0.414261</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a01n01</td>\n",
       "      <td>2020-04-29 00:05:00</td>\n",
       "      <td>1018.244275</td>\n",
       "      <td>520.101104</td>\n",
       "      <td>566.0</td>\n",
       "      <td>2229.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a01n01</td>\n",
       "      <td>2020-04-29 00:10:00</td>\n",
       "      <td>853.309963</td>\n",
       "      <td>378.770956</td>\n",
       "      <td>570.0</td>\n",
       "      <td>2135.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a01n01</td>\n",
       "      <td>2020-04-29 00:15:00</td>\n",
       "      <td>780.363636</td>\n",
       "      <td>333.054690</td>\n",
       "      <td>568.0</td>\n",
       "      <td>2136.0</td>\n",
       "      <td>22.325758</td>\n",
       "      <td>0.469547</td>\n",
       "      <td>22.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a01n01</td>\n",
       "      <td>2020-04-29 00:20:00</td>\n",
       "      <td>671.046154</td>\n",
       "      <td>210.677923</td>\n",
       "      <td>557.0</td>\n",
       "      <td>1521.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  hostname           timestamp  total_power.mean  total_power.std  \\\n",
       "0   a01n01 2020-04-29 00:00:00       1077.569811       540.773446   \n",
       "1   a01n01 2020-04-29 00:05:00       1018.244275       520.101104   \n",
       "2   a01n01 2020-04-29 00:10:00        853.309963       378.770956   \n",
       "3   a01n01 2020-04-29 00:15:00        780.363636       333.054690   \n",
       "4   a01n01 2020-04-29 00:20:00        671.046154       210.677923   \n",
       "\n",
       "   total_power.min  total_power.max  ambient.mean  ambient.std  ambient.min  \\\n",
       "0            566.0           2231.0     22.781132     0.414261         22.0   \n",
       "1            566.0           2229.0     23.000000     0.000000         23.0   \n",
       "2            570.0           2135.0     23.000000     0.000000         23.0   \n",
       "3            568.0           2136.0     22.325758     0.469547         22.0   \n",
       "4            557.0           1521.0     22.000000     0.000000         22.0   \n",
       "\n",
       "   ambient.max  \n",
       "0         23.0  \n",
       "1         23.0  \n",
       "2         23.0  \n",
       "3         23.0  \n",
       "4         22.0  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-communications",
   "metadata": {},
   "source": [
    "# Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "wireless-anderson",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(jobs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "virgin-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
